import requests, json, ast
from NER import sents2prompt
from tqdm import tqdm
from utils import timeit, write_json
import time
from pathlib import Path
import os
from multi_threading import mt_Woker
from functools import partial
import re
from zhipuai import ZhipuAI

from logger import Logger
from datetime import datetime
from config import Config

config = Config()
START_TIME = datetime.now().strftime("%Y-%m-%d %H:%M")
LOG_FILE_PATH = config.log_file_path
_logger = Logger(log_file_path=LOG_FILE_PATH)
MAX_RETRY = 10
error_file = open(config.error_file_path,"a")

@timeit
def call_llm_api(prompt, api_key, max_retries=10, timeout=200):
    """
    The function `call_llm_api` sends a POST request to ChatGPT API endpoint with given prompt.
    
    :param prompt: The `prompt` parameter in the `call_llm_api` function is a string that represents the
    input text or message that will be used as a prompt for the language model API. This prompt will be
    used to generate a response from the model specified in the `data` dictionary
    :param api_key: The `api_key` parameter in the `call_llm_api` function is used to authenticate your
    API requests. 
    :param max_retries: maximum number of times the API call will be retried in case of a failure.
    :param timeout: The `timeout` parameter in the `call_llm_api` function specifies the maximum number
    of seconds the function will wait for a response from the API before raising a timeout error. 
    :return: The function `call_llm_api` returns the completion message generated by the API if the
    response status code is 200. If there is an error in the response, it returns an error message
    including the status code and response text. If there is a network-related error during the request,
    it returns a message indicating a network error. If the maximum number of retries is reached without
    a successful response,
    """
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    data = {
        # 'model': 'gpt-3.5-turbo-1106',
        'model': 'gpt-4',
        'temperature': 0.2,
        "messages": [            {
                "role": "user",
                "content": prompt
            }
        ]
    }

    for _ in range(max_retries):
        try:
            response = requests.post('https://api.kwwai.top/v1/chat/completions', headers=headers, json=data, timeout=timeout)
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                return f"Error: {response.status_code}, {response.text}"
        except requests.Timeout:
            print("Request timed out, retrying...")
        except requests.RequestException as e:
            # 其他网络相关错误可以在这里处理
            return f"Network error: {e}"

    return "Error: Maximum retries reached"

# def call_llm_api(prompt, api_key, max_retries=10, timeout=200):
#     client = ZhipuAI(api_key="1209462db651581cc1109a0f085d71bb.hYnWiG3sIXj8KRDO") # 填写您自己的APIKey
#     response = client.chat.completions.create(
#     model="glm-4", # 填写需要调用的模型名称
#     messages=[
#         {"role": "user", "content": prompt
#          }
#     ],)
#     print(response.choices[0].message)
#     return response.choices[0].message.content

# with open('/data/shenyuge/lingyue-data-process/doc2json/output2.json', 'r') as f:
#     data = json.load(f)

# sentences = [(item['sentence index'], item['keycontent']) for item in data if item['paper category']=='行业深度报告']
# prompt = sents2prompt(sentences)
# with open('prompts.json', 'w', encoding="utf-8") as f:
#     json.dump(prompt, f, ensure_ascii=False, indent=4)
# assert len(sentences)==len(prompt)

# prompt = """在句子'从行业层面来看，白酒的动销、库存、批价、回款节奏等是判断酒企业绩与景气度的关键指标'中，有没有出现["并列", "递进", "因果", "转折", "让步", "时间顺序", "条件", "举例", "目的", "假设", "逻辑推理"]列表里的关系？如果有是哪几种？"""
# prompt = """在‘从行业层面来看，白酒的动销、库存、批价、回款节奏等是判断酒企业绩与景气度的关键指标’这句话中，词语两两之间是否有["并列", "递进", "因果", "转折", "让步", "时间顺序", "条件", "举例", "目的", "假设", "逻辑推理"]中的一个关系,请注意要是列表中的关系"""
# prompt = f"以下问题只输出字典结果，不用解释，不要复述：第一步，利用COT方法生成出多角度的分析方向；第二步，根据每个方向，从‘第四轮白酒周期：2019年初至今’这句话中提炼出多个KG triplets"
# response = call_llm_api(prompt)
# print(response)
# result = {}
# for i, sent in enumerate(sentences):
#     unrelated_prompt = call_llm_api(prompt[i])
#     result[sent[1]] = unrelated_prompt
#     print(unrelated_prompt)
# print(result)

def __add_logic_chain(data, json_directory=None, output_path=None, write_to_file=False):
    if not data:
        try:
            with open(json_directory, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except:
            print("Invalid json file, please enter valid directory.")
    # for item in tqdm(data):
        # prompt = sents2prompt(item['keycontent'])
    for si, prompt in data:
        response = call_llm_api(prompt, api_key = 'sk-QeiIJwcjqnhybuSeBbC0F27eEc0b42529a4410194b362bBb')
        try:
            response = ast.literal_eval(response.replace('，', ','))
            data +=list(response)
        except:
            print(response)
            continue

    if write_to_file:
        assert output_path is not None
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    return data


def _add_logic_chain_test(data, json_directory=None, output_path=None, write_to_file=False):
    """
    This Python function processes data by calling an API with prompts, handling errors, and add logic chains to
    data with responses, with an option to write the output to a file.
    
    :param data: The `data` parameter in the `add_logic_chain_test` function is the input data that the
    function will process. It can be a list, dictionary, or any other data structure that contains the
    information needed for the logic chain test
    :param json_directory: The `json_directory` parameter in the `add_logic_chain_test` function is used
    to specify the directory path of a JSON file that contains data. This function will attempt to read
    the data from this JSON file if the `data` parameter is empty or not provided. If the JSON file is
    invalid
    :param output_path: path where the modified `data` will be saved as a JSON file if `write_to_file` is
    set to `True`. 
    :param write_to_file: whether the processed data should be written to a file or not. If
    `write_to_file` is set to `True`, the function will write the processed data to a file specified by
    the, defaults to False (optional)
    :return: The function `add_logic_chain_test` returns the modified `data` after processing and adding
    logic chains to it. If `write_to_file` is set to True, the function will also write the updated
    `data` to a file specified by `output_path`.
    """
    if not data:
        try:
            with open(json_directory, 'r', encoding='utf-8') as f:
                data = json.load(f)
                f.close()
        except:
            print("Invalid json file, please enter valid directory.")
    previous_error = False
    work_list = {i:0 for i in range(874, len(data), 1)}
    work_count = 0
    while len(work_list)>0:
        for work_index in list(work_list.keys())[:]:
            print(f"currently is working on {work_index}/{len(work_list)}", end="\r")
            item = data[work_index]
            # sent = item['keycontent'].replace('\n', '')
            # if len(sent) > 100:
            #     sent = sent[:100]
            # prompt = sents2prompt(sent)
            prompt = item.get('prompt', None)
            # if not prompt:
            #     break
            response = call_llm_api(prompt, api_key = 'sk-QeiIJwcjqnhybuSeBbC0F27eEc0b42529a4410194b362bBb')
            try:
                response = ast.literal_eval(response.replace('，', ','))
                print('index {}: {}\n'.format(work_index, response))
                item['logicchain'] +=list(response)
                work_list.pop(work_index)
                # if previous_error:
                #     previous_error = False
            except:
                work_list[work_index] += 1
                if work_list[work_index]>=MAX_RETRY:
                    work_list.pop(work_index)
                    error_file.write(f"{work_index}\n")
                    print(f"{work_index} is removed")
                # if response.status_code>=400:
                time.sleep(2)
            work_count += 1
            if work_count//30 == 1:
                with open('/data/shenyuge/lingyue-data-process/logic_chain/gpt_log.json', "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=4)
                    f.close()
                work_count = 0
    error_file.close()
    if write_to_file:
        assert output_path is not None
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            f.close()
    return data

def get_response(item, case_index, logger):
    """
    The function `get_response` retrieves an api response parse the response and logs
    success or failure based on a given case index.
    
    :param item: The `item` parameter seems to be a dictionary containing information related to a
    specific case or item. It may have keys such as 'prompt', 'logicchain', etc. The function
    `get_response` takes this `item` dictionary, a `case_index`, and a `logger` as parameters
    :param case_index: Sentence index number. 
    :param logger: a logger object from `logging` module
    :return: The function `get_response` returns the `item` dictionary after processing the prompt and
    updating the 'logicchain' key with the response from the API. If successful, it logs the information
    and returns the updated `item`. If there is an exception during the processing, it logs the error
    and returns None.
    """
    prompt = item.get('prompt', None)
    if prompt is None:
        return
    response = call_llm_api(prompt, api_key = config.api_key)
    try:
        response = ast.literal_eval(response.replace('，', ','))
        logger.info(f"successful on case: {case_index}")
        item['logicchain'] +=list(response)
    except Exception as e:
        logger.error(f"failed on case: {case_index}",
                     e)
        return None
    return item

def get_sent_logic(sent):
    try:
        response = call_llm_api(sents2prompt(sent),api_key=config.api_key)
        response = ast.literal_eval(response.replace('，', ','))
        # logger.info(f"successful on case: {case_index}")
        logic_chain = list(response)
        return logic_chain
    except:
        return get_sent_logic(sent)

def save_response_multi_threading(output_folder, n_save, output_queue = None, nthread=None, end_value=None):
    """
    This function saves api responses to JSON files in a multi-threaded manner based on specified
    parameters.
    
    :param output_folder: The `output_folder` parameter is the directory path where the output files
    will be saved
    :param n_save: the maximum number of responses to save in each partition file before creating a new file. Once the
    number of responses reaches `n_save`, a new partition file is created to store additional responses
    :param output_queue: queue used for storing output responses that need to be saved. 
    :param nthread: Number of threads will be concurrently processing and saving the responses to the output files
    :param end_value: The `end_value` parameter is used to determine when to stop saving responses. When
    the `output_response` is equal to the `end_value`, it indicates that the responses have reached the
    end and the saving process should stop
    """
    result = []
    n_partition = 1
    n_ends = 0
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    while True:
        try:
            output_response = output_queue.get(block = False)
        except:
            time.sleep(0.1)
            continue

        if output_response:
            if not output_response == end_value:
                result += [output_response]
            else:
                n_ends += 1
        if n_save and len(result) >= n_save: 
            write_json(Path(output_folder)/f"save_file_partition{n_partition}.json", result)
            result = []
            n_partition += 1
        if n_ends == nthread:
            break
    file_name = f"save_file_partition{n_partition}.json" if n_save else "save_result.json"
    write_json(Path(output_folder)/file_name, result)

def add_logic_chain(propmt_directory, output_directory):
    with open(propmt_directory, 'r', encoding='utf-8') as f:
        data = json.load(f)
    output_folder = f"{output_directory}/{START_TIME}"

    worker = mt_Woker(
        exec_func = partial(get_response, logger = _logger),
        post_func = partial(save_response_multi_threading, output_folder = output_folder, n_save = 300),
        n_thread = 30
    )
    for index, item in  enumerate(data):
        worker.add_item((item, index))
    
    worker.run()

if __name__ == "__main__":
    json_directory = '/data/shenyuge/lingyue-data-process/logic_chain/prompts.json'
    with open('/data/shenyuge/lingyue-data-process/logic_chain/prompts.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    from multi_threading import mt_Woker
    from functools import partial
    import re

    saved_index = []
    f = open(r"/data/shenyuge/logs/log_file_for_2024-04-07 10:44.log", "r", encoding="utf-8")
    while line:= f.readline():
        re_expresssion = re.search(r"failed on case: (\d+)",line)
        if re_expresssion:
            saved_index += [int(re_expresssion.groups()[0])]
    output_folder = f"/data/shenyuge/lingyue-data-process/logic_chain/multi_process_test/{START_TIME}"
    worker = mt_Woker(
        exec_func = partial(get_response, logger = _logger),
        post_func = partial(save_response_multi_threading, output_folder = output_folder, n_save = 300),
        n_thread = 30
    )
    for index, item in  enumerate(data):
        if index in saved_index:
            worker.add_item((item, index))
    
    worker.run()
# if __name__=='__main__':
#     json_directory = '/data/shenyuge/lingyue-data-process/logic_chain/prompts_maotai.json'
#     output_path = '/data/shenyuge/lingyue-data-process/logic_chain/gpt_result_maotai.json'
#     with open('/data/shenyuge/lingyue-data-process/logic_chain/prompts_maotai.json', 'r', encoding='utf-8') as f:
#         data = json.load(f)
#         f.close()
#     result = add_logic_chain_test(data, None, output_path, True)